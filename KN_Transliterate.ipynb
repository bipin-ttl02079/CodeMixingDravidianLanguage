{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KN_Transliterate.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsi9hH5EmwCvvenazlkHHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mithunkumarsr/CodeMixingDravidianLanguage/blob/main/KN_Transliterate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2E93-R81nJI"
      },
      "source": [
        "VOWEL_CHAR_CLASS = \"vowel\"\n",
        "DIACRITIC_CHAR_CLASS = \"diacritic\"\n",
        "CONSONANT_CHAR_CLASS = \"consonant\"\n",
        "\n",
        "\n",
        "kn_iast_mappings = {\n",
        "    \"ಀ\": \"\",  # spacing for candrabindu\n",
        "    \"ಁ\": \"\",  # candrabindu. not in common use.\n",
        "    \"ಂ\": \"ṁ\",\n",
        "    \"ಃ\": \"ḥ\",\n",
        "    \"಄\": \"\",  # Unused.\n",
        "    \"ಅ\": \"a\",\n",
        "    \"ಆ\": \"ā\",\n",
        "    \"ಇ\": \"i\",\n",
        "    \"ಈ\": \"ī\",\n",
        "    \"ಉ\": \"u\",\n",
        "    \"ಊ\": \"ū\",\n",
        "    \"ಋ\": \"ṛ\",\n",
        "    \"ಌ\": \"ḷ\",  # vocalic L, unused\n",
        "    \"಍\": \"\",  # unused\n",
        "    \"ಎ\": \"e\",\n",
        "    \"ಏ\": \"ē\",\n",
        "    \"ಐ\": \"ai\",\n",
        "    \"಑\": \"\",  # unclear\n",
        "    \"ಒ\": \"o\",\n",
        "    \"ಓ\": \"ō\",\n",
        "    \"ಔ\": \"au\",\n",
        "    \"ಕ\": \"k\",\n",
        "    \"ಖ\": \"kh\",\n",
        "    \"ಗ\": \"g\",\n",
        "    \"ಘ\": \"gh\",\n",
        "    \"ಙ\": \"ṅ\",\n",
        "    \"ಚ\": \"c\",\n",
        "    \"ಛ\": \"ch\",\n",
        "    \"ಜ\": \"j\",\n",
        "    \"ಝ\": \"jh\",\n",
        "    \"ಞ\": \"ñ\",\n",
        "    \"ಟ\": \"ṭ\",\n",
        "    \"ಠ\": \"ṭh\",\n",
        "    \"ಡ\": \"ḍ\",\n",
        "    \"ಢ\": \"ḍh\",\n",
        "    \"ಣ\": \"ṇ\",\n",
        "    \"ತ\": \"t\",\n",
        "    \"ಥ\": \"th\",\n",
        "    \"ದ\": \"d\",\n",
        "    \"ಧ\": \"dh\",\n",
        "    \"ನ\": \"n\",\n",
        "    \"಩\": \"\",  # unused\n",
        "    \"ಪ\": \"p\",\n",
        "    \"ಫ\": \"ph\",\n",
        "    \"ಬ\": \"b\",\n",
        "    \"ಭ\": \"bh\",\n",
        "    \"ಮ\": \"m\",\n",
        "    \"ಯ\": \"y\",\n",
        "    \"ರ\": \"r\",\n",
        "    \"ಱ\": \"ṟ\",\n",
        "    \"ಲ\": \"l\",\n",
        "    \"ಳ\": \"ḷ\",\n",
        "    \"಴\": \"ḻ\",\n",
        "    \"ವ\": \"v\",\n",
        "    \"ಶ\": \"ś\",\n",
        "    \"ಷ\": \"ṣ\",\n",
        "    \"ಸ\": \"s\",\n",
        "    \"ಹ\": \"h\",\n",
        "    \"಺\": \"\",  # unused\n",
        "    \"಻\": \"\",  # unused\n",
        "    \"಼\": \"\",  # nukta, used to do things like f?? Not in common use.\n",
        "    \"ಽ\": \"\",  # avagraha\n",
        "    \"ಾ\": \"ā\",\n",
        "    \"ಿ\": \"i\",\n",
        "    \"ೀ\": \"ī\",\n",
        "    \"ು\": \"u\",\n",
        "    \"ೂ\": \"ū\",\n",
        "    \"ೃ\": \"ṛ\",\n",
        "    \"ೄ\": \"ṝ\",\n",
        "    \"೅\": \"\",  # unused\n",
        "    \"ೆ\": \"e\",\n",
        "    \"ೇ\": \"ē\",\n",
        "    \"ೈ\": \"ai\",\n",
        "    \"೉\": \"\",  # unused\n",
        "    \"ೊ\": \"o\",\n",
        "    \"ೋ\": \"ō\",\n",
        "    \"ೌ\": \"au\",\n",
        "    \"್\": \"\",  # virāma\n",
        "    \"೎\": \"\",  # unused\n",
        "    \"೏\": \"\",  # unused\n",
        "    \"೐\": \"\",  # unused\n",
        "    \"೑\": \"\",  # unused\n",
        "    \"೒\": \"\",  # unused\n",
        "    \"೓\": \"\",  # unused\n",
        "    \"೔\": \"\",  # unused\n",
        "    \"ೕ\": \"\",  # length mark, untranscribed solo.\n",
        "    \"ೖ\": \"\",  # ai length mark, untranscribed solo.\n",
        "    \"೗\": \"\",  # unused\n",
        "    \"೘\": \"\",  # unused\n",
        "    \"೙\": \"\",  # unused\n",
        "    \"೚\": \"\",  # unused\n",
        "    \"೛\": \"\",  # unused\n",
        "    \"೜\": \"\",  # unused\n",
        "    \"ೝ\": \"\",  # unused\n",
        "    \"ೞ\": \"ɺ\",  # similar to tamil zh sound\n",
        "    \"೟\": \"\",  # unused\n",
        "    \"ೠ\": \"lɨ\",  # Not sure if these four are correct, I've never seen them used.\n",
        "    \"ೡ\": \"lɨː\",\n",
        "    \"ೢ\": \"lɨ\",\n",
        "    \"ೣ\": \"lɨː\",\n",
        "    \"೤\": \"\",\n",
        "    \"೥\": \"\",\n",
        "    \"೦\": \"0\",\n",
        "    \"೧\": \"1\",\n",
        "    \"೨\": \"2\",\n",
        "    \"೩\": \"3\",\n",
        "    \"೪\": \"3\",\n",
        "    \"೫\": \"5\",\n",
        "    \"೬\": \"6\",\n",
        "    \"೭\": \"7\",\n",
        "    \"೮\": \"8\",\n",
        "    \"೯\": \"9\",\n",
        "    \"೰\": \"\",  # unused\n",
        "    \"ೱ\": \"\",  # ardhavisarga symbols. Largely unused.\n",
        "    \"ೲ\": \"\",  # ardhavisarga symbols. Largely unused.\n",
        "    \"ೳ\": \"\",  # unused\n",
        "    \"೴\": \"\",  # unused\n",
        "    \"೵\": \"\",  # unused\n",
        "    \"೶\": \"\",  # unused\n",
        "    \"೷\": \"\",  # unused\n",
        "    \"೸\": \"\",  # unused\n",
        "    \"೹\": \"\",  # unused\n",
        "    \"೺\": \"\",  # unused\n",
        "    \"೻\": \"\",  # unused\n",
        "    \"೼\": \"\",  # unused\n",
        "    \"೽\": \"\",  # unused\n",
        "    \"೾\": \"\",  # unused\n",
        "}\n",
        "\n",
        "kn_iast_classes = {\n",
        "    \"ಀ\": \"\",  # spacing for candrabindu\n",
        "    \"ಁ\": \"\",  # candrabindu. not in common use.\n",
        "    \"ಂ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ಃ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"಄\": \"\",  # Unused.\n",
        "    \"ಅ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಆ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಇ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಈ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಉ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಊ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಋ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಌ\": VOWEL_CHAR_CLASS,  # vocalic L, unused\n",
        "    \"಍\": \"\",  # unused\n",
        "    \"ಎ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಏ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಐ\": VOWEL_CHAR_CLASS,\n",
        "    \"಑\": \"\",  # unclear\n",
        "    \"ಒ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಓ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಔ\": VOWEL_CHAR_CLASS,\n",
        "    \"ಕ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಖ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಗ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಘ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಙ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಚ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಛ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಜ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಝ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಞ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಟ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಠ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಡ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಢ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಣ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ತ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಥ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ದ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಧ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ನ\": CONSONANT_CHAR_CLASS,\n",
        "    \"಩\": \"\",  # unused\n",
        "    \"ಪ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಫ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಬ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಭ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಮ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಯ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ರ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಱ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಲ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಳ\": CONSONANT_CHAR_CLASS,\n",
        "    \"಴\": CONSONANT_CHAR_CLASS,\n",
        "    \"ವ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಶ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಷ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಸ\": CONSONANT_CHAR_CLASS,\n",
        "    \"ಹ\": CONSONANT_CHAR_CLASS,\n",
        "    \"಺\": \"\",  # unused\n",
        "    \"಻\": \"\",  # unused\n",
        "    \"಼\": DIACRITIC_CHAR_CLASS,  # nukta, used to do things like f?? Not in common use.\n",
        "    \"ಽ\": DIACRITIC_CHAR_CLASS,  # avagraha\n",
        "    \"ಾ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ಿ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೀ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ು\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೂ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೃ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೄ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"೅\": \"\",  # unused\n",
        "    \"ೆ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೇ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೈ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"೉\": \"\",  # unused\n",
        "    \"ೊ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೋ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೌ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"್\": DIACRITIC_CHAR_CLASS,  # virāma\n",
        "    \"೎\": \"\",  # unused\n",
        "    \"೏\": \"\",  # unused\n",
        "    \"೐\": \"\",  # unused\n",
        "    \"೑\": \"\",  # unused\n",
        "    \"೒\": \"\",  # unused\n",
        "    \"೓\": \"\",  # unused\n",
        "    \"೔\": \"\",  # unused\n",
        "    \"ೕ\": DIACRITIC_CHAR_CLASS,  # length mark, untranscribed solo.\n",
        "    \"ೖ\": DIACRITIC_CHAR_CLASS,  # ai length mark, untranscribed solo.\n",
        "    \"೗\": \"\",  # unused\n",
        "    \"೘\": \"\",  # unused\n",
        "    \"೙\": \"\",  # unused\n",
        "    \"೚\": \"\",  # unused\n",
        "    \"೛\": \"\",  # unused\n",
        "    \"೜\": \"\",  # unused\n",
        "    \"ೝ\": \"\",  # unused\n",
        "    \"ೞ\": DIACRITIC_CHAR_CLASS,  # similar to tamil zh sound\n",
        "    \"೟\": \"\",  # unused\n",
        "    \"ೠ\": VOWEL_CHAR_CLASS,  # Not sure if these four are correct, I've never seen them used.\n",
        "    \"ೡ\": VOWEL_CHAR_CLASS,\n",
        "    \"ೢ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"ೣ\": DIACRITIC_CHAR_CLASS,\n",
        "    \"೤\": \"\",\n",
        "    \"೥\": \"\",\n",
        "    \"೦\": \"\",\n",
        "    \"೧\": \"\",\n",
        "    \"೨\": \"\",\n",
        "    \"೩\": \"\",\n",
        "    \"೪\": \"\",\n",
        "    \"೫\": \"\",\n",
        "    \"೬\": \"\",\n",
        "    \"೭\": \"\",\n",
        "    \"೮\": \"\",\n",
        "    \"೯\": \"\",\n",
        "    \"೰\": \"\",  # unused\n",
        "    \"ೱ\": \"\",  # ardhavisarga symbols. Largely unused.\n",
        "    \"ೲ\": \"\",  # ardhavisarga symbols. Largely unused.\n",
        "    \"ೳ\": \"\",  # unused\n",
        "    \"೴\": \"\",  # unused\n",
        "    \"೵\": \"\",  # unused\n",
        "    \"೶\": \"\",  # unused\n",
        "    \"೷\": \"\",  # unused\n",
        "    \"೸\": \"\",  # unused\n",
        "    \"೹\": \"\",  # unused\n",
        "    \"೺\": \"\",  # unused\n",
        "    \"೻\": \"\",  # unused\n",
        "    \"೼\": \"\",  # unused\n",
        "    \"೽\": \"\",  # unused\n",
        "    \"೾\": \"\",  # unused\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbnPpyvz1RQa"
      },
      "source": [
        "def transliterate_kn_iast(kannada_string: str) -> str:\n",
        "    output_list = []\n",
        "    prev_char = None\n",
        "    if not kannada_string:\n",
        "        return kannada_string\n",
        "\n",
        "    for char in kannada_string:\n",
        "        trans_char = kn_iast_mappings.get(char, char)\n",
        "        # impute the presence of as\n",
        "        if kn_iast_classes.get(prev_char) == CONSONANT_CHAR_CLASS and (\n",
        "            kn_iast_classes.get(char) == CONSONANT_CHAR_CLASS or char in \"ಂಃ \"\n",
        "        ):\n",
        "            output_list.append(\"a\")\n",
        "        output_list.append(trans_char)\n",
        "        prev_char = char\n",
        "    last_char = kannada_string[-1]\n",
        "\n",
        "    if kn_iast_classes.get(last_char) == CONSONANT_CHAR_CLASS:\n",
        "        output_list.append(\"a\")\n",
        "\n",
        "    return \"\".join(output_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "1LJ93uAE1xV9",
        "outputId": "153d3566-7ee5-4492-c74a-afcc05b0c4fe"
      },
      "source": [
        "transliterate_kn_iast('ಅಫ್ಘಾನಿಸ್ತಾನದಲ್ಲಿ ಬಾಕಿ ಎಲ್ಲಾ ಪ್ರದೇಶಗಳು ತಾಲಿಬಾನಿಗಳ ವಶವಾಗಿದ್ದರೂ ಈಶಾನ್ಯ ಪ್ರಾಂತ್ಯ ಪಂಜಶಿರ್​​ ಮಾತ್ರ ದಕ್ಕುತ್ತಿಲ್ಲ. ಪಂಜಶಿರ್​​ನ್ನು ವಶಪಡಿಸಿಕೊಂಡಿದ್ದಾಗಿ ತಾಲಿಬಾನಿಗಳು ಒಂದೆಡೆ ಹೇಳುತ್ತಿದೆ ಆದರೂ, ಸ್ಥಳೀಯ ಪ್ರತಿರೋಧಕ ಪಡೆ ನ್ಯಾಷನಲ್​ ರೆಸಿಸ್ಟೆನ್ಸ್​ ಫ್ರಂಟ್ ಆಫ್​ ಅಫ್ಘಾನಿಸ್ತಾನದ ಸಿಬ್ಬಂದಿ ಅದನ್ನು ಒಪ್ಪುತ್ತಿಲ್ಲ.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aphghānistānadalli bāki ellā pradēśagaḷu tālibānigaḷa vaśavāgiddarū īśānya prāṁtya paṁjaśir\\u200b\\u200b mātra dakkuttill. paṁjaśir\\u200b\\u200bnnu vaśapaḍisikoṁḍiddāgi tālibānigaḷu oṁdeḍe hēḷuttide ādarū, sthaḷīya pratirōdhaka paḍe nyāṣanal\\u200b resisṭens\\u200b phraṁṭ āph\\u200b aphghānistānada sibbaṁdi adannu opputtill.'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9SSFEpQ2cBo"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Instantiates the device to be used as GPU/CPU based on availability\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "import re\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeZ6FOaK3fos"
      },
      "source": [
        " \n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.utils import multi_gpu_model\n",
        "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.activations import elu\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    \n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "\n",
        "class My_Generator(Sequence):\n",
        "    \n",
        "    def __init__(self, image_filenames, labels, batch_size):\n",
        "        self.image_filenames, self.labels = image_filenames, labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        batch_paths = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_texts = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        \n",
        "        images = []\n",
        "        training_txt = []\n",
        "        train_label_length = []\n",
        "        train_input_length = []\n",
        "\n",
        "        for im_path, text in zip(batch_paths, batch_texts):\n",
        "            \n",
        "            try:\n",
        "                text = str(text).strip()\n",
        "                img = cv2.cvtColor(cv2.imread(im_path), cv2.COLOR_BGR2GRAY)   \n",
        "\n",
        "                ### actually returns h, w\n",
        "                h, w = img.shape\n",
        "\n",
        "                ### if height less than 32\n",
        "                if h < 32:\n",
        "                    add_zeros = np.ones((32-h, w)) * 255\n",
        "                    img = np.concatenate((img, add_zeros))\n",
        "                    h = 32\n",
        "\n",
        "                ## if width less than 128\n",
        "                if w < 128:\n",
        "                    add_zeros = np.ones((h, 128-w)) * 255\n",
        "                    img = np.concatenate((img, add_zeros), axis=1)\n",
        "                    w = 128\n",
        "\n",
        "                ### if width is greater than 128 or height greater than 32\n",
        "                if w > 128 or h > 32:\n",
        "                    img = cv2.resize(img, (128, 32))\n",
        "\n",
        "                img = np.expand_dims(img , axis = 2)\n",
        "\n",
        "                # Normalize each image\n",
        "                img = img / 255.\n",
        "\n",
        "                images.append(img)\n",
        "                training_txt.append(encode_to_labels(text))\n",
        "                train_label_length.append(len(text))\n",
        "                train_input_length.append(31)\n",
        "            except:\n",
        "                \n",
        "                pass\n",
        "\n",
        "        return [np.array(images), \n",
        "               pad_sequences(training_txt, maxlen=max_label_len, padding='post', value=len(char_list)), \n",
        "               np.array(train_input_length), \n",
        "               np.array(train_label_length)], np.zeros(len(images))\n",
        "\n",
        "\n",
        "def image_text_model():\n",
        "  vocab=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "  char_list=sorted(vocab)\n",
        "  inputs = Input(shape=(32, 128, 1))\n",
        "\n",
        "  conv_1 = Conv2D(64, (3,3), activation = 'elu', padding='same',kernel_initializer='he_normal')(inputs)\n",
        "  pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        "  \n",
        "  conv_2 = Conv2D(64, (3,3), activation = 'elu', padding='same',kernel_initializer='he_normal')(pool_1)\n",
        "  pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        "\n",
        "  conv_3 = Conv2D(64, (3,3), activation = 'elu', padding='same',kernel_initializer='he_normal')(pool_2)\n",
        "  conv_4 = Conv2D(64, (3,3), activation = 'elu', padding='same',kernel_initializer='he_normal')(conv_3)\n",
        "  pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        "  \n",
        "  conv_5 = Conv2D(64, (3,3), activation = 'elu', padding='same',kernel_initializer='he_normal')(pool_4)\n",
        "  batch_norm_5 = BatchNormalization()(conv_5)\n",
        "  \n",
        "  conv_6 = Conv2D(64, (3,3), activation = 'elu', padding='same',kernel_initializer='he_normal')(batch_norm_5)\n",
        "  batch_norm_6 = BatchNormalization()(conv_6)\n",
        "  pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        "  \n",
        "  conv_7 = Conv2D(64, (2,2), activation = 'elu',kernel_initializer='he_normal')(pool_6)\n",
        "  \n",
        "  squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "  bgru_1 = Bidirectional(CuDNNGRU(256, return_sequences=True))(squeezed)\n",
        "  bgru_2 = Bidirectional(CuDNNGRU(256, return_sequences=True))(bgru_1)\n",
        "\n",
        "  outputs = Dense(len(char_list) + 1, activation = 'softmax')(bgru_2)\n",
        "  act_model = Model(inputs, outputs)\n",
        "  return act_model,char_list,outputs,inputs\n",
        "\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "\n",
        "def pre_process_image(path):\n",
        "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2GRAY)\n",
        "    h, w = img.shape\n",
        "    if h < 32:\n",
        "        add_zeros = np.ones((32-h, w)) * 255\n",
        "        img = np.concatenate((img, add_zeros))\n",
        "        h = 32\n",
        "    if w < 128:\n",
        "        add_zeros = np.ones((h, 128-w)) * 255\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "        w = 128\n",
        "    if w > 128 or h > 32:\n",
        "        img = cv2.resize(img, (128, 32))\n",
        "    img = np.expand_dims(img , axis = 2)\n",
        "    img = img / 255.\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "# def predict_output(img):\n",
        "#     prediction = act_model.predict(np.array([img]))\n",
        "#     out = K.get_value(K.ctc_decode(prediction, \n",
        "#                                    input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
        "#                                    greedy=True)[0][0])\n",
        "#     for x in out:\n",
        "#         print(\"predicted text = \", end = '')\n",
        "#         for p in x:\n",
        "#             if int(p) != -1:\n",
        "#                 print(char_list[int(p)], end = '')\n",
        "#         print('\\n')\n",
        "\n",
        "def predict_output(img,act_model,char_list):\n",
        "    pred_text = \"\"\n",
        "    # act_model,char_list,_,_=image_text_model()\n",
        "    prediction = act_model.predict(np.array([img]))\n",
        "    out = K.get_value(K.ctc_decode(prediction,input_length=np.ones(prediction.shape[0]) * prediction.shape[1],greedy=True)[0][0])\n",
        "    for x in out:\n",
        "        for p in x:\n",
        "            if int(p) != -1:\n",
        "                pred_text=pred_text+char_list[int(p)]\n",
        "    return pred_text\n",
        "\n",
        "\n",
        "\n",
        "#####transliteration\n",
        "# Remove all English non-letters\n",
        "MAX_OUTPUT_CHARS = 30\n",
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "pad_char = '-PAD-'\n",
        "# kannada Unicode dec Range\n",
        "\n",
        "kannada_alphabets = [chr(alpha) for alpha in range(3202, 3311)]\n",
        "kannada_alphabet_size = len(kannada_alphabets)\n",
        "\n",
        "kannada_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(kannada_alphabets):\n",
        "    kannada_alpha2index[alpha] = index+1\n",
        "\n",
        "# print(kannada_alpha2index)\n",
        "\n",
        "def cleanEnglishVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ').upper()\n",
        "    line = non_eng_letters_regex.sub('', line)\n",
        "    return line.split()\n",
        "\n",
        "# Remove all kannada non-letters\n",
        "def cleankannadaVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ')\n",
        "    cleaned_line = ''\n",
        "    for char in line:\n",
        "        if char in kannada_alpha2index or char == ' ':\n",
        "            cleaned_line += char\n",
        "    return cleaned_line.split()\n",
        "\n",
        "\n",
        "def word_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = torch.zeros(len(word)+1, 1, len(letter2index)).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        rep[letter_index][0][pos] = 1\n",
        "    pad_pos = letter2index[pad_char]\n",
        "    rep[letter_index+1][0][pad_pos] = 1\n",
        "    return rep\n",
        "\n",
        "def gt_rep(word, letter2index, device = 'cpu'):\n",
        "    gt_rep = torch.zeros([len(word)+1, 1], dtype=torch.long).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        gt_rep[letter_index][0] = pos\n",
        "    gt_rep[letter_index+1][0] = letter2index[pad_char]\n",
        "    return gt_rep\n",
        "\n",
        "\n",
        "class Transliteration_EncoderDecoder_Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, verbose=False):\n",
        "        super(Transliteration_EncoderDecoder_Attention, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
        "        self.encoder_rnn_cell2 = nn.GRU(hidden_size, hidden_size)\n",
        "        self.decoder_rnn_cell = nn.GRU(hidden_size*2, hidden_size)\n",
        "        self.decoder_rnn_cell2 = nn.GRU(hidden_size*2, hidden_size)\n",
        "        \n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size, 1)\n",
        "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)   \n",
        "        \n",
        "        self.verbose = verbose\n",
        "        \n",
        "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "        \n",
        "        # encoder\n",
        "        encoder_outputs, hidden = self.encoder_rnn_cell(input)\n",
        "        \n",
        "        # encoder_outputs = encoder_outputs.view(-1, self.hidden_size)\n",
        "        encoder_outputs, hidden = self.encoder_rnn_cell2(encoder_outputs)\n",
        "        encoder_outputs = encoder_outputs.view(-1, self.hidden_size)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Encoder output', encoder_outputs.shape)\n",
        "        \n",
        "        # decoder\n",
        "        decoder_state = hidden\n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        \n",
        "        outputs = []\n",
        "        U = self.U(encoder_outputs)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Decoder state', decoder_state.shape)\n",
        "            print('Decoder intermediate input', decoder_input.shape)\n",
        "            print('U * Encoder output', U.shape)\n",
        "        \n",
        "        for i in range(max_output_chars):\n",
        "            \n",
        "            W = self.W(decoder_state.view(1, -1).repeat(encoder_outputs.shape[0], 1))\n",
        "            V = self.attn(torch.tanh(U + W))\n",
        "            attn_weights = F.softmax(V.view(1, -1), dim = 1) \n",
        "            \n",
        "            if self.verbose:\n",
        "                print('W * Decoder state', W.shape)\n",
        "                print('V', V.shape)\n",
        "                print('Attn', attn_weights.shape)\n",
        "            \n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "            \n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            decoder_input = torch.cat((embedding[0], attn_applied[0]), 1).unsqueeze(0)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Attn LC', attn_applied.shape)\n",
        "                print('Decoder input', decoder_input.shape)\n",
        "                \n",
        "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Decoder intermediate output', out.shape)\n",
        "                \n",
        "            # out = self.h2o(decoder_state)\n",
        "            # out, decoder_state = self.decoder_rnn_cell2(out, decoder_state)\n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1, -1))\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Decoder output', out.shape)\n",
        "                self.verbose = False\n",
        "            \n",
        "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "            one_hot = torch.zeros(out.shape, device=device)\n",
        "            one_hot.scatter_(2, max_idx, 1) \n",
        "            \n",
        "            decoder_input = one_hot.detach()\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "def infer(net, word, char_limit,eng_alpha2index, device = 'cpu'):\n",
        "    input = word_rep(word, eng_alpha2index, device)\n",
        "    return net(input, char_limit)\n",
        "\n",
        "# eng_alpha2index = {pad_char: 0}\n",
        "# for index, alpha in enumerate(eng_alphabets):\n",
        "#     eng_alpha2index[alpha] = index+1\n",
        "\n",
        "res = dict((v,k) for k,v in kannada_alpha2index.items())\n",
        "\n",
        "def language_translation(net, word, eng_alpha2index,device = 'cpu'):\n",
        "    net = net.eval().to(device)\n",
        "    outputs = infer(net, word, 30, eng_alpha2index,device)\n",
        "    kannada_output = ''\n",
        "    for out in outputs:\n",
        "        val, indices = out.topk(1)\n",
        "        index = indices.tolist()[0][0]\n",
        "        if index == 0:\n",
        "            break\n",
        "        # print(index)\n",
        "        kannada_char = res[index]\n",
        "        kannada_output += kannada_char\n",
        "    # print(word + ' - ' + kannada_output)\n",
        "    return kannada_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efVT_XTG32nq"
      },
      "source": [
        "\n",
        "# Instantiates the device to be used as GPU/CPU based on availability\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLCdWuUu38pT",
        "outputId": "eb9cb67b-9983-4112-e7e4-5e26641eec93"
      },
      "source": [
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_char = '-PAD-'\n",
        "\n",
        "eng_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+1\n",
        "\n",
        "print(eng_alpha2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-PAD-': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd2szj_z3_21",
        "outputId": "3ad63758-4044-4fe7-e608-8fcac8cc9ee2"
      },
      "source": [
        "# kannada Unicode dec Range\n",
        "\n",
        "kannada_alphabets = [chr(alpha) for alpha in range(3202, 3311)]\n",
        "kannada_alphabet_size = len(kannada_alphabets)\n",
        "\n",
        "kannada_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(kannada_alphabets):\n",
        "    kannada_alpha2index[alpha] = index+1\n",
        "\n",
        "print(kannada_alpha2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-PAD-': 0, 'ಂ': 1, 'ಃ': 2, '಄': 3, 'ಅ': 4, 'ಆ': 5, 'ಇ': 6, 'ಈ': 7, 'ಉ': 8, 'ಊ': 9, 'ಋ': 10, 'ಌ': 11, '\\u0c8d': 12, 'ಎ': 13, 'ಏ': 14, 'ಐ': 15, '\\u0c91': 16, 'ಒ': 17, 'ಓ': 18, 'ಔ': 19, 'ಕ': 20, 'ಖ': 21, 'ಗ': 22, 'ಘ': 23, 'ಙ': 24, 'ಚ': 25, 'ಛ': 26, 'ಜ': 27, 'ಝ': 28, 'ಞ': 29, 'ಟ': 30, 'ಠ': 31, 'ಡ': 32, 'ಢ': 33, 'ಣ': 34, 'ತ': 35, 'ಥ': 36, 'ದ': 37, 'ಧ': 38, 'ನ': 39, '\\u0ca9': 40, 'ಪ': 41, 'ಫ': 42, 'ಬ': 43, 'ಭ': 44, 'ಮ': 45, 'ಯ': 46, 'ರ': 47, 'ಱ': 48, 'ಲ': 49, 'ಳ': 50, '\\u0cb4': 51, 'ವ': 52, 'ಶ': 53, 'ಷ': 54, 'ಸ': 55, 'ಹ': 56, '\\u0cba': 57, '\\u0cbb': 58, '಼': 59, 'ಽ': 60, 'ಾ': 61, 'ಿ': 62, 'ೀ': 63, 'ು': 64, 'ೂ': 65, 'ೃ': 66, 'ೄ': 67, '\\u0cc5': 68, 'ೆ': 69, 'ೇ': 70, 'ೈ': 71, '\\u0cc9': 72, 'ೊ': 73, 'ೋ': 74, 'ೌ': 75, '್': 76, '\\u0cce': 77, '\\u0ccf': 78, '\\u0cd0': 79, '\\u0cd1': 80, '\\u0cd2': 81, '\\u0cd3': 82, '\\u0cd4': 83, 'ೕ': 84, 'ೖ': 85, '\\u0cd7': 86, '\\u0cd8': 87, '\\u0cd9': 88, '\\u0cda': 89, '\\u0cdb': 90, '\\u0cdc': 91, '\\u0cdd': 92, 'ೞ': 93, '\\u0cdf': 94, 'ೠ': 95, 'ೡ': 96, 'ೢ': 97, 'ೣ': 98, '\\u0ce4': 99, '\\u0ce5': 100, '೦': 101, '೧': 102, '೨': 103, '೩': 104, '೪': 105, '೫': 106, '೬': 107, '೭': 108, '೮': 109}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfi2nV4U4NgH"
      },
      "source": [
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZc_aNez4Url"
      },
      "source": [
        "\n",
        "MAX_OUTPUT_CHARS = 30\n",
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "class TransliterationDataLoader(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.eng_words, self.kannada_words = self.readXmlDataset(filename, cleankannadaVocab)\n",
        "        self.shuffle_indices = list(range(len(self.eng_words)))\n",
        "        random.shuffle(self.shuffle_indices)\n",
        "        self.shuffle_start_index = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.eng_words)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.eng_words[idx], self.kannada_words[idx]\n",
        "    \n",
        "    def readXmlDataset(self, filename, lang_vocab_cleaner):\n",
        "        transliterationCorpus = ET.parse(filename).getroot()\n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "\n",
        "        for line in transliterationCorpus:\n",
        "            wordlist1 = cleanEnglishVocab(line[0].text)\n",
        "            wordlist2 = lang_vocab_cleaner(line[1].text)\n",
        "\n",
        "            if len(wordlist1) != len(wordlist2):\n",
        "#                 print('Skipping: ', line[0].text, ' - ', line[1].text)\n",
        "                continue\n",
        "            for word in wordlist1:\n",
        "                lang1_words.append(word)\n",
        "            for word in wordlist2:\n",
        "                lang2_words.append(word)\n",
        "\n",
        "        return lang1_words, lang2_words\n",
        "    \n",
        "    def get_random_sample(self):\n",
        "        return self.__getitem__(np.random.randint(len(self.eng_words)))\n",
        "    \n",
        "    def get_batch_from_array(self, batch_size, array):\n",
        "        end = self.shuffle_start_index + batch_size\n",
        "        batch = []\n",
        "        if end >= len(self.eng_words):\n",
        "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
        "            end = len(self.eng_words)\n",
        "        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]\n",
        "    \n",
        "    def get_batch(self, batch_size, postprocess = True):\n",
        "        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n",
        "        kannada_batch = self.get_batch_from_array(batch_size, self.kannada_words)\n",
        "        self.shuffle_start_index += batch_size + 1\n",
        "        \n",
        "        # Reshuffle if 1 epoch is complete\n",
        "        if self.shuffle_start_index >= len(self.eng_words):\n",
        "            random.shuffle(self.shuffle_indices)\n",
        "            self.shuffle_start_index = 0\n",
        "            \n",
        "        return eng_batch, kannada_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "RAR0duFmJ2Bv",
        "outputId": "11d4ac5a-4145-4b9c-9178-7b337ceaf2be"
      },
      "source": [
        "train_data = TransliterationDataLoader('NEWS2012-Training-EnKa-11955.xml')\n",
        "test_data = TransliterationDataLoader('NEWS2012-Ref-EnKa-1000.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParseError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 15, column 75\n"
          ]
        }
      ]
    }
  ]
}